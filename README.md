# Лабораторная работа #3. РАЗРАБОТКА СИСТЕМЫ МАШИННОГО ОБУЧЕНИЯ.
Отчет по лабораторной работе #3 выполнил(а):
- Ли Александр Альбертович
- Х21IT_AI-01BL

Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨


## Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity. В данной лабораторной работе мы создадим ML-агент и будем тренировать
нейросеть, задача которой будет заключаться в управлении шаром. Задача шара заключается в том, чтобы оставаясь на плоскости находить кубик, смещающийся в заданном случайном диапазоне координат.

## Задание 1
### Реализовать систему машинного обучения в связке Python – Google-Sheets – Unity.

Установка необходимых средств и настройка системы машинного обучения выполнились успешно.

![sphere (1)](https://user-images.githubusercontent.com/78469125/195573894-7acb9afa-5400-48a2-9e03-5685da1e5e6b.gif)

Установка дополнительных одинаковых полей позволяют ускорить обучение ввиду увеличению итераций.

![3 things](https://user-images.githubusercontent.com/78469125/195575458-60b692ac-a1a4-4709-bb2f-a5ae36713b9a.PNG)

Шаг Step стал увеличиваться быстрее с 54 полями:

![40k rotations](https://user-images.githubusercontent.com/78469125/195575645-f3f278f9-2d1e-437d-b29e-d23bc85aed8c.PNG)

После, возвращаясь к единой модели, заметно улучшилась ловкость шара: он стал быстрее и точнее догонять куб. Движение стало плавным. Таким образом, машинное обучение позволило шару с меньшей задержкой достигать цели.



## Задание 2

### Подробно опишите каждую строку файла конфигурации нейронной сети, доступного в папке с файлами проекта по ссылке. Самостоятельно найдите информацию о компонентах Decision Requester, Behavior Parameters, добавленных на сфере.

```
behaviors:
  RollerBall:
  # proximal policy optimization. Данная функция использует нейронную сеть для приближения к идеальной функции, сопоставляемая с наилучшим действием агента
    trainer_type: ppo 
    # hyperparameters запуск обучения с задаваемыми параметрами
    hyperparameters:
     # batch_size количество опытов (описано ниже)
      batch_size: 10
     # buffer_size соответствует количеству опыта (наблюдения, действия и награды), необходимое для сбора до повторного обучения или обновления модели
      buffer_size: 100 
     # learning_rate соответствует силе алгоритма для поиска минимума функции потерь. Обычно это значение следует уменьшить, если тренировка нестабильна, а вознаграждение не увеличивается последовательно
      learning_rate: 3.0e-4 
     # beta соответствует силе параметра entropy regularization, отвечающая за рандомизацию РРО. Это позволяет агентам корректно исследовать пространство действий. Увеличение этого значения обеспечит выполнение большего количества случайных действий
      beta: 5.0e-4
     # epsilon соответствует допустимому порогу расхождения между старой и новой политиками при обновлении с градиентным спуском (методом нахождения локального минимума или максимума функции). Установка этого значения небольшим приведет к более стабильным обновлениям, но также замедлит процесс обучения
      epsilon: 0.2
     # lambd соответствует параметру лямбда, используемому при расчете Обобщенной оценки преимущества (GAE). Это можно рассматривать как то, насколько агент полагается на свою текущую оценку стоимости при расчете обновленной оценки стоимости. Низкие значения соответствуют тому, что вы больше полагаетесь на текущую оценку ценности (что может быть большой погрешностью), а высокие значения соответствуют тому, что вы больше полагаетесь на фактические вознаграждения, полученные в окружающей среде (что может быть высокой дисперсией). Параметр обеспечивает компромисс между ними, и правильное значение может привести к более стабильному процессу обучения
      lambd: 0.99
     # num_epoch — это количество проходов через буфер опыта во время градиентного спуска
      num_epoch: 3
     # тип алгоритма
      learning_rate_schedule: linear
    network_settings:
     # normalize соответствует тому, применяется ли нормализация к входным данным векторного наблюдения. Эта нормализация основана на текущем среднем значении и дисперсии векторного наблюдения. Нормализация может быть полезна в случаях со сложными задачами непрерывного управления, но может быть вредна при более простых задачах дискретного управления.
      normalize: false
     # hidden_units соответствует количеству единиц в каждом полностью подключенном слое нейронной сети. Для простых задач, где правильным действием является простая комбинация входных данных наблюдения, значение должно быть небольшим. Для задач, где действие представляет собой очень сложное взаимодействие между переменными наблюдения, значение должно быть больше.
      hidden_units: 128
     # num_layers соответствует количеству скрытых слоев, присутствующих после ввода наблюдения. Для простых задач меньшее количество слоев, скорее всего, будет обучаться быстрее и эффективнее. Для более сложных задач управления может потребоваться больше уровней.
      num_layers: 2
    reward_signals:
      extrinsic:
     # gamma соответствует коэффициенту будущих вознаграждений. Это можно рассматривать как то, насколько далеко в будущем агент должен заботиться о возможных вознаграждениях
        gamma: 0.99
        strength: 1.0
    max_steps: 500000
    # time_horizon соответствует количеству шагов сбора опыта для каждого агента до добавления его в буфер опыта. Когда этот предел достигается до окончания эпизода, используется оценка стоимости  для прогнозирования общего ожидаемого вознаграждения от текущего состояния агента. Таким образом, этот параметр является компромиссом между менее предвзятой, но более высокой оценкой дисперсии (long time horizon) и более предвзятой, но менее разнообразной оценкой (short time horizon)
    time_horizon: 64
    # summary_freq соответствует частоте обновления опыта агента
    summary_freq: 10000
```

## Задание 3
### Доработайте сцену и обучите ML-Agent таким образом, чтобы шар перемещался между двумя кубами разного цвета. Кубы должны, как и в первом задании, случайно изменять координаты на плоскости.



## Выводы

Удалось настроить API к гугл-таблицам от PyCharm, было интересно разобрать изначально данный код на последующие задачи. Один из выводов состоит в том, что Unity, а точнее С#, очень привередлив к коду, ввиду чего требовалась постоянная модификация скрипта, сравнения с дефолтными данными, дебаг, и нервы.


| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
